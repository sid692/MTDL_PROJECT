{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 5256696,
          "sourceType": "datasetVersion",
          "datasetId": 3041726
        }
      ],
      "dockerImageVersionId": 30559,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'cifake-real-and-ai-generated-synthetic-images:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3041726%2F5256696%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T004044Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dac4d3f21856b42675ce2bda2f1c1bba3ba132072ca6704dc2951dcbaaa1f8bc0caa0ee0ad0dac63530f8691e7bb37d0c4d9419d71aa8ffef4f92ce43e4c02cac6c57502a7701209d0ab313f388d3652950a76745f409d3a7fd722cb0f858b216c57eef90ae9bbad8cef89ffab64101b017944a4f5c7481ec4670eac24ddc21dcdd9e25ca0ae24f2157ef8336bd0370b715b429489bfa0b844dc642ee525554b16fa8a7e39e710aa5534ff84e0186f981ec208689f1a249f92835b6b056e3ad7a15412624408ca4742d8200df76b2862c4590d57252f9cef11546c9e72f5ce6ae19f6a9e2848ef63f3fda78866a7342ad7c5c39f838077907d5ae17986d74bb00'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "s_0n-JrHv-7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73302ef2-c092-4478-9560-1e4eb78aba1c"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cifake-real-and-ai-generated-synthetic-images, 109625224 bytes compressed\n",
            "[==================================================] 109625224 bytes downloaded\n",
            "Downloaded and uncompressed: cifake-real-and-ai-generated-synthetic-images\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries, load and transform data"
      ],
      "metadata": {
        "id": "YWJXWUD-v-7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary Python packages using pip\n",
        "\n",
        "# Use the 'pip' command to install packages\n",
        "# The '-q' flag stands for 'quiet,' which means it will suppress most output, making the installation process less verbose\n",
        "# We're installing the following packages:\n",
        "# - 'evaluate': This package is likely used for evaluation purposes, but the specific functionality is not clear from this line alone\n",
        "# - 'transformers': This package is commonly used for natural language processing tasks, such as working with pre-trained language models like BERT or GPT\n",
        "# - 'datasets': This package provides easy access to various datasets commonly used in machine learning and natural language processing tasks\n",
        "# - 'mlflow': MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models\n",
        "\n",
        "# Note: Before running this code, make sure you have Python and pip installed on your system.\n",
        "# Also, ensure you have an internet connection since pip will download and install these packages from PyPI (Python Package Index).\n",
        "!pip install -U -q evaluate transformers datasets>=2.14.5 accelerate>=0.27 mlflow 2>/dev/null"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:13:49.79249Z",
          "iopub.execute_input": "2024-04-06T08:13:49.792872Z",
          "iopub.status.idle": "2024-04-06T08:14:34.946041Z",
          "shell.execute_reply.started": "2024-04-06T08:13:49.792839Z",
          "shell.execute_reply": "2024-04-06T08:14:34.944974Z"
        },
        "trusted": true,
        "id": "KnUtb1xHv-70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imblearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLONEZHZ5gey",
        "outputId": "609817a3-4adb-4f57-f666-9b49ad1108d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.10/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UzGlYH75oWW",
        "outputId": "16864232-06a9-4520-e6e4-2a90657303d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0+cpu)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "j7LMmdQ86Uep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bf0045d-01ba-4322-b0ed-45099d3ec8e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2024.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries and modules\n",
        "import warnings  # Import the 'warnings' module for handling warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Ignore warnings during execution\n",
        "\n",
        "import gc  # Import the 'gc' module for garbage collection\n",
        "import numpy as np  # Import NumPy for numerical operations\n",
        "import pandas as pd  # Import Pandas for data manipulation\n",
        "import itertools  # Import 'itertools' for iterators and looping\n",
        "from collections import Counter  # Import 'Counter' for counting elements\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib for data visualization\n",
        "from sklearn.metrics import (  # Import various metrics from scikit-learn\n",
        "    accuracy_score,  # For calculating accuracy\n",
        "    roc_auc_score,  # For ROC AUC score\n",
        "    confusion_matrix,  # For confusion matrix\n",
        "    classification_report,  # For classification report\n",
        "    f1_score  # For F1 score\n",
        ")\n",
        "\n",
        "# Import custom modules and classes\n",
        "from imblearn.over_sampling import RandomOverSampler # import RandomOverSampler\n",
        "import accelerate # Import the 'accelerate' module\n",
        "import evaluate  # Import the 'evaluate' module\n",
        "from datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\n",
        "from transformers import (  # Import various modules from the Transformers library\n",
        "    TrainingArguments,  # For training arguments\n",
        "    Trainer,  # For model training\n",
        "    ViTImageProcessor,  # For processing image data with ViT models\n",
        "    ViTForImageClassification,  # ViT model for image classification\n",
        "    DefaultDataCollator  # For collating data in the default way\n",
        ")\n",
        "import torch  # Import PyTorch for deep learning\n",
        "from torch.utils.data import DataLoader  # For creating data loaders\n",
        "from torchvision.transforms import (  # Import image transformation functions\n",
        "    CenterCrop,  # Center crop an image\n",
        "    Compose,  # Compose multiple image transformations\n",
        "    Normalize,  # Normalize image pixel values\n",
        "    RandomRotation,  # Apply random rotation to images\n",
        "    RandomResizedCrop,  # Crop and resize images randomly\n",
        "    RandomHorizontalFlip,  # Apply random horizontal flip\n",
        "    RandomAdjustSharpness,  # Adjust sharpness randomly\n",
        "    Resize,  # Resize images\n",
        "    ToTensor  # Convert images to PyTorch tensors\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:14:34.94815Z",
          "iopub.execute_input": "2024-04-06T08:14:34.948444Z",
          "iopub.status.idle": "2024-04-06T08:14:48.651449Z",
          "shell.execute_reply.started": "2024-04-06T08:14:34.94841Z",
          "shell.execute_reply": "2024-04-06T08:14:48.650593Z"
        },
        "trusted": true,
        "id": "o-ysqHAwv-71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary module from the Python Imaging Library (PIL).\n",
        "from PIL import ImageFile\n",
        "\n",
        "# Enable the option to load truncated images.\n",
        "# This setting allows the PIL library to attempt loading images even if they are corrupted or incomplete.\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:14:48.652515Z",
          "iopub.execute_input": "2024-04-06T08:14:48.653467Z",
          "iopub.status.idle": "2024-04-06T08:14:48.658291Z",
          "shell.execute_reply.started": "2024-04-06T08:14:48.653436Z",
          "shell.execute_reply": "2024-04-06T08:14:48.657361Z"
        },
        "trusted": true,
        "id": "QA0Uust_v-72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use https://huggingface.co/docs/datasets/image_load for reference\n",
        "\n",
        "# Import necessary libraries\n",
        "image_dict = {}\n",
        "\n",
        "# Define the list of file names\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "# Initialize empty lists to store file names and labels\n",
        "file_names = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through all image files in the specified directory\n",
        "for file in sorted((Path('/kaggle/input/cifake-real-and-ai-generated-synthetic-images/').glob('*/*/*.*'))):\n",
        "    label = str(file).split('/')[-2]  # Extract the label from the file path\n",
        "    labels.append(label)  # Add the label to the list\n",
        "    file_names.append(str(file))  # Add the file path to the list\n",
        "\n",
        "# Print the total number of file names and labels\n",
        "print(len(file_names), len(labels))\n",
        "\n",
        "# Create a pandas dataframe from the collected file names and labels\n",
        "df = pd.DataFrame.from_dict({\"image\": file_names, \"label\": labels})\n",
        "print(df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:14:48.661017Z",
          "iopub.execute_input": "2024-04-06T08:14:48.661319Z",
          "iopub.status.idle": "2024-04-06T08:14:59.185403Z",
          "shell.execute_reply.started": "2024-04-06T08:14:48.661294Z",
          "shell.execute_reply": "2024-04-06T08:14:59.184505Z"
        },
        "trusted": true,
        "id": "Dv4WyN5yv-73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:14:59.186367Z",
          "iopub.execute_input": "2024-04-06T08:14:59.1867Z",
          "iopub.status.idle": "2024-04-06T08:14:59.207514Z",
          "shell.execute_reply.started": "2024-04-06T08:14:59.186675Z",
          "shell.execute_reply": "2024-04-06T08:14:59.206133Z"
        },
        "trusted": true,
        "id": "KdSXxAShv-74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:14:59.208698Z",
          "iopub.execute_input": "2024-04-06T08:14:59.208982Z",
          "iopub.status.idle": "2024-04-06T08:14:59.445364Z",
          "shell.execute_reply.started": "2024-04-06T08:14:59.208958Z",
          "shell.execute_reply": "2024-04-06T08:14:59.444414Z"
        },
        "trusted": true,
        "id": "IYidMULNv-75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random oversampling of minority class\n",
        "# 'y' contains the target variable (label) we want to predict\n",
        "y = df[['label']]\n",
        "\n",
        "# Drop the 'label' column from the DataFrame 'df' to separate features from the target variable\n",
        "df = df.drop(['label'], axis=1)\n",
        "\n",
        "# Create a RandomOverSampler object with a specified random seed (random_state=83)\n",
        "ros = RandomOverSampler(random_state=83)\n",
        "\n",
        "# Use the RandomOverSampler to resample the dataset by oversampling the minority class\n",
        "# 'df' contains the feature data, and 'y_resampled' will contain the resampled target variable\n",
        "df, y_resampled = ros.fit_resample(df, y)\n",
        "\n",
        "# Delete the original 'y' variable to save memory as it's no longer needed\n",
        "del y\n",
        "\n",
        "# Add the resampled target variable 'y_resampled' as a new 'label' column in the DataFrame 'df'\n",
        "df['label'] = y_resampled\n",
        "\n",
        "# Delete the 'y_resampled' variable to save memory as it's no longer needed\n",
        "del y_resampled\n",
        "\n",
        "# Perform garbage collection to free up memory used by discarded variables\n",
        "gc.collect()\n",
        "\n",
        "print(df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:14:59.446526Z",
          "iopub.execute_input": "2024-04-06T08:14:59.44691Z",
          "iopub.status.idle": "2024-04-06T08:15:00.421366Z",
          "shell.execute_reply.started": "2024-04-06T08:14:59.446873Z",
          "shell.execute_reply": "2024-04-06T08:15:00.420374Z"
        },
        "trusted": true,
        "id": "0N3k8d51v-76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from a Pandas DataFrame.\n",
        "dataset = Dataset.from_pandas(df).cast_column(\"image\", Image())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:00.422396Z",
          "iopub.execute_input": "2024-04-06T08:15:00.422665Z",
          "iopub.status.idle": "2024-04-06T08:15:00.535015Z",
          "shell.execute_reply.started": "2024-04-06T08:15:00.422642Z",
          "shell.execute_reply": "2024-04-06T08:15:00.534033Z"
        },
        "trusted": true,
        "id": "cTJZnsdnv-76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first image in the dataset\n",
        "dataset[0][\"image\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:00.536126Z",
          "iopub.execute_input": "2024-04-06T08:15:00.53639Z",
          "iopub.status.idle": "2024-04-06T08:15:00.564061Z",
          "shell.execute_reply.started": "2024-04-06T08:15:00.536366Z",
          "shell.execute_reply": "2024-04-06T08:15:00.563192Z"
        },
        "trusted": true,
        "id": "DvMlxWnpv-77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting a subset of elements from the 'labels' list using slicing.\n",
        "# The slicing syntax [:5] selects elements from the beginning up to (but not including) the 5th element.\n",
        "# This will give us the first 5 elements of the 'labels' list.\n",
        "# The result will be a new list containing these elements.\n",
        "labels_subset = labels[:5]\n",
        "\n",
        "# Printing the subset of labels to inspect the content.\n",
        "print(labels_subset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:00.567885Z",
          "iopub.execute_input": "2024-04-06T08:15:00.568203Z",
          "iopub.status.idle": "2024-04-06T08:15:00.573504Z",
          "shell.execute_reply.started": "2024-04-06T08:15:00.568175Z",
          "shell.execute_reply": "2024-04-06T08:15:00.572547Z"
        },
        "trusted": true,
        "id": "Xgp_I9OZv-77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of unique labels by converting 'labels' to a set and then back to a list\n",
        "labels_list = ['REAL', 'FAKE'] #list(set(labels))\n",
        "\n",
        "# Initialize empty dictionaries to map labels to IDs and vice versa\n",
        "label2id, id2label = dict(), dict()\n",
        "\n",
        "# Iterate over the unique labels and assign each label an ID, and vice versa\n",
        "for i, label in enumerate(labels_list):\n",
        "    label2id[label] = i  # Map the label to its corresponding ID\n",
        "    id2label[i] = label  # Map the ID to its corresponding label\n",
        "\n",
        "# Print the resulting dictionaries for reference\n",
        "print(\"Mapping of IDs to Labels:\", id2label, '\\n')\n",
        "print(\"Mapping of Labels to IDs:\", label2id)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:00.574818Z",
          "iopub.execute_input": "2024-04-06T08:15:00.575156Z",
          "iopub.status.idle": "2024-04-06T08:15:00.585457Z",
          "shell.execute_reply.started": "2024-04-06T08:15:00.575123Z",
          "shell.execute_reply": "2024-04-06T08:15:00.584502Z"
        },
        "trusted": true,
        "id": "SvW2fsWlv-78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating classlabels to match labels to IDs\n",
        "ClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n",
        "\n",
        "# Mapping labels to IDs\n",
        "def map_label2id(example):\n",
        "    example['label'] = ClassLabels.str2int(example['label'])\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(map_label2id, batched=True)\n",
        "\n",
        "# Casting label column to ClassLabel Object\n",
        "dataset = dataset.cast_column('label', ClassLabels)\n",
        "\n",
        "# Splitting the dataset into training and testing sets using an 60-40 split ratio.\n",
        "dataset = dataset.train_test_split(test_size=0.4, shuffle=True, stratify_by_column=\"label\")\n",
        "\n",
        "# Extracting the training data from the split dataset.\n",
        "train_data = dataset['train']\n",
        "\n",
        "# Extracting the testing data from the split dataset.\n",
        "test_data = dataset['test']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:00.586904Z",
          "iopub.execute_input": "2024-04-06T08:15:00.587239Z",
          "iopub.status.idle": "2024-04-06T08:15:01.149437Z",
          "shell.execute_reply.started": "2024-04-06T08:15:00.587209Z",
          "shell.execute_reply": "2024-04-06T08:15:01.14846Z"
        },
        "trusted": true,
        "id": "GsP8csf0v-78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pre-trained ViT model string\n",
        "model_str = \"amunchet/rorshark-vit-base\" #'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "# Create a processor for ViT model input from the pre-trained model\n",
        "processor = ViTImageProcessor.from_pretrained(model_str)\n",
        "\n",
        "# Retrieve the image mean and standard deviation used for normalization\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "\n",
        "# Get the size (height) of the ViT model's input images\n",
        "size = processor.size[\"height\"]\n",
        "print(\"Size: \", size)\n",
        "\n",
        "# Define a normalization transformation for the input images\n",
        "normalize = Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "# Define a set of transformations for training data\n",
        "_train_transforms = Compose(\n",
        "    [\n",
        "        Resize((size, size)),             # Resize images to the ViT model's input size\n",
        "        RandomRotation(90),               # Apply random rotation\n",
        "        RandomAdjustSharpness(2),         # Adjust sharpness randomly\n",
        "        ToTensor(),                       # Convert images to tensors\n",
        "        normalize                         # Normalize images using mean and std\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define a set of transformations for validation data\n",
        "_val_transforms = Compose(\n",
        "    [\n",
        "        Resize((size, size)),             # Resize images to the ViT model's input size\n",
        "        ToTensor(),                       # Convert images to tensors\n",
        "        normalize                         # Normalize images using mean and std\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define a function to apply training transformations to a batch of examples\n",
        "def train_transforms(examples):\n",
        "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "# Define a function to apply validation transformations to a batch of examples\n",
        "def val_transforms(examples):\n",
        "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:01.151025Z",
          "iopub.execute_input": "2024-04-06T08:15:01.151329Z",
          "iopub.status.idle": "2024-04-06T08:15:01.423216Z",
          "shell.execute_reply.started": "2024-04-06T08:15:01.151303Z",
          "shell.execute_reply": "2024-04-06T08:15:01.422327Z"
        },
        "trusted": true,
        "id": "YKOZrFfXv-78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the transforms for the training data\n",
        "train_data.set_transform(train_transforms)\n",
        "\n",
        "# Set the transforms for the test/validation data\n",
        "test_data.set_transform(val_transforms)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:01.424387Z",
          "iopub.execute_input": "2024-04-06T08:15:01.424685Z",
          "iopub.status.idle": "2024-04-06T08:15:01.437094Z",
          "shell.execute_reply.started": "2024-04-06T08:15:01.424659Z",
          "shell.execute_reply": "2024-04-06T08:15:01.43615Z"
        },
        "trusted": true,
        "id": "ikvM-s2Hv-79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a collate function that prepares batched data for model training.\n",
        "def collate_fn(examples):\n",
        "    # Stack the pixel values from individual examples into a single tensor.\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "\n",
        "    # Convert the label strings in examples to corresponding numeric IDs using label2id dictionary.\n",
        "    labels = torch.tensor([example['label'] for example in examples])\n",
        "\n",
        "    # Return a dictionary containing the batched pixel values and labels.\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:01.438442Z",
          "iopub.execute_input": "2024-04-06T08:15:01.438799Z",
          "iopub.status.idle": "2024-04-06T08:15:01.454125Z",
          "shell.execute_reply.started": "2024-04-06T08:15:01.438768Z",
          "shell.execute_reply": "2024-04-06T08:15:01.453188Z"
        },
        "trusted": true,
        "id": "As6Ouk-Av-79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load, train, and evaluate model"
      ],
      "metadata": {
        "id": "WL7sciQmv-79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ViTForImageClassification model from a pretrained checkpoint with a specified number of output labels.\n",
        "model = ViTForImageClassification.from_pretrained(model_str, num_labels=len(labels_list))\n",
        "\n",
        "# Configure the mapping of class labels to their corresponding indices for later reference.\n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id\n",
        "\n",
        "# Calculate and print the number of trainable parameters in millions for the model.\n",
        "print(model.num_parameters(only_trainable=True) / 1e6)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:01.455349Z",
          "iopub.execute_input": "2024-04-06T08:15:01.455705Z",
          "iopub.status.idle": "2024-04-06T08:15:03.361383Z",
          "shell.execute_reply.started": "2024-04-06T08:15:01.455673Z",
          "shell.execute_reply": "2024-04-06T08:15:03.36038Z"
        },
        "trusted": true,
        "id": "mVa4mmUrv-79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the accuracy metric from a module named 'evaluate'\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define a function 'compute_metrics' to calculate evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    # Extract model predictions from the evaluation prediction object\n",
        "    predictions = eval_pred.predictions\n",
        "\n",
        "    # Extract true labels from the evaluation prediction object\n",
        "    label_ids = eval_pred.label_ids\n",
        "\n",
        "    # Calculate accuracy using the loaded accuracy metric\n",
        "    # Convert model predictions to class labels by selecting the class with the highest probability (argmax)\n",
        "    predicted_labels = predictions.argmax(axis=1)\n",
        "\n",
        "    # Calculate accuracy score by comparing predicted labels to true labels\n",
        "    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']\n",
        "\n",
        "    # Return the computed accuracy as a dictionary with the key \"accuracy\"\n",
        "    return {\n",
        "        \"accuracy\": acc_score\n",
        "    }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:03.362534Z",
          "iopub.execute_input": "2024-04-06T08:15:03.362864Z",
          "iopub.status.idle": "2024-04-06T08:15:04.087338Z",
          "shell.execute_reply.started": "2024-04-06T08:15:03.362838Z",
          "shell.execute_reply": "2024-04-06T08:15:04.086495Z"
        },
        "trusted": true,
        "id": "gpBCUIHHv-79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the name of the evaluation metric to be used during training and evaluation.\n",
        "metric_name = \"accuracy\"\n",
        "\n",
        "# Define the name of the model, which will be used to create a directory for saving model checkpoints and outputs.\n",
        "model_name = \"ai_vs_real_image_detection\"\n",
        "\n",
        "# Define the number of training epochs for the model.\n",
        "num_train_epochs = 2\n",
        "\n",
        "# Create an instance of TrainingArguments to configure training settings.\n",
        "args = TrainingArguments(\n",
        "    # Specify the directory where model checkpoints and outputs will be saved.\n",
        "    output_dir=model_name,\n",
        "\n",
        "    # Specify the directory where training logs will be stored.\n",
        "    logging_dir='./logs',\n",
        "\n",
        "    # Define the evaluation strategy, which is performed at the end of each epoch.\n",
        "    evaluation_strategy=\"epoch\",\n",
        "\n",
        "    # Set the learning rate for the optimizer.\n",
        "    learning_rate=1e-6,\n",
        "\n",
        "    # Define the batch size for training on each device.\n",
        "    per_device_train_batch_size=64,\n",
        "\n",
        "    # Define the batch size for evaluation on each device.\n",
        "    per_device_eval_batch_size=32,\n",
        "\n",
        "    # Specify the total number of training epochs.\n",
        "    num_train_epochs=num_train_epochs,\n",
        "\n",
        "    # Apply weight decay to prevent overfitting.\n",
        "    weight_decay=0.02,\n",
        "\n",
        "    # Set the number of warm-up steps for the learning rate scheduler.\n",
        "    warmup_steps=50,\n",
        "\n",
        "    # Disable the removal of unused columns from the dataset.\n",
        "    remove_unused_columns=False,\n",
        "\n",
        "    # Define the strategy for saving model checkpoints (per epoch in this case).\n",
        "    save_strategy='epoch',\n",
        "\n",
        "    # Load the best model at the end of training.\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    # Limit the total number of saved checkpoints to save space.\n",
        "    save_total_limit=1,\n",
        "\n",
        "    # Specify that training progress should not be reported .\n",
        "    report_to=\"none\"  # log to none\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:04.088452Z",
          "iopub.execute_input": "2024-04-06T08:15:04.088733Z",
          "iopub.status.idle": "2024-04-06T08:15:04.173909Z",
          "shell.execute_reply.started": "2024-04-06T08:15:04.088709Z",
          "shell.execute_reply": "2024-04-06T08:15:04.173113Z"
        },
        "trusted": true,
        "id": "EWDy73JFv-7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Trainer instance for fine-tuning a language model.\n",
        "\n",
        "# - `model`: The pre-trained language model to be fine-tuned.\n",
        "# - `args`: Configuration settings and hyperparameters for training.\n",
        "# - `train_dataset`: The dataset used for training the model.\n",
        "# - `eval_dataset`: The dataset used for evaluating the model during training.\n",
        "# - `data_collator`: A function that defines how data batches are collated and processed.\n",
        "# - `compute_metrics`: A function for computing custom evaluation metrics.\n",
        "# - `tokenizer`: The tokenizer used for processing text data.\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:04.175066Z",
          "iopub.execute_input": "2024-04-06T08:15:04.17535Z",
          "iopub.status.idle": "2024-04-06T08:15:04.42152Z",
          "shell.execute_reply.started": "2024-04-06T08:15:04.175326Z",
          "shell.execute_reply": "2024-04-06T08:15:04.420729Z"
        },
        "trusted": true,
        "id": "9t5Gb0wEv-7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the pre-training model's performance on a test dataset.\n",
        "# This function calculates various metrics such as accuracy, loss, etc.,\n",
        "# to assess how well the model is performing on unseen data.\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:15:04.422541Z",
          "iopub.execute_input": "2024-04-06T08:15:04.422828Z",
          "iopub.status.idle": "2024-04-06T08:33:42.026866Z",
          "shell.execute_reply.started": "2024-04-06T08:15:04.422803Z",
          "shell.execute_reply": "2024-04-06T08:33:42.025937Z"
        },
        "trusted": true,
        "id": "ppDJTr3Hv-7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training the model using the trainer object.\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T08:33:42.028171Z",
          "iopub.execute_input": "2024-04-06T08:33:42.028526Z",
          "iopub.status.idle": "2024-04-06T09:57:10.926867Z",
          "shell.execute_reply.started": "2024-04-06T08:33:42.028491Z",
          "shell.execute_reply": "2024-04-06T09:57:10.925905Z"
        },
        "trusted": true,
        "id": "UYykSBkHv-7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the post-training model's performance on the validation or test dataset.\n",
        "# This function computes various evaluation metrics like accuracy, loss, etc.\n",
        "# and provides insights into how well the model is performing.\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T09:57:10.928203Z",
          "iopub.execute_input": "2024-04-06T09:57:10.928907Z",
          "iopub.status.idle": "2024-04-06T10:08:19.263102Z",
          "shell.execute_reply.started": "2024-04-06T09:57:10.928868Z",
          "shell.execute_reply": "2024-04-06T10:08:19.26218Z"
        },
        "trusted": true,
        "id": "bqoVqS4dv-7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained 'trainer' to make predictions on the 'test_data'.\n",
        "outputs = trainer.predict(test_data)\n",
        "\n",
        "# Print the metrics obtained from the prediction outputs.\n",
        "print(outputs.metrics)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:08:19.264466Z",
          "iopub.execute_input": "2024-04-06T10:08:19.264844Z",
          "iopub.status.idle": "2024-04-06T10:19:36.010034Z",
          "shell.execute_reply.started": "2024-04-06T10:08:19.26481Z",
          "shell.execute_reply": "2024-04-06T10:19:36.009082Z"
        },
        "trusted": true,
        "id": "_bnUzORSv-7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the true labels from the model outputs\n",
        "y_true = outputs.label_ids\n",
        "\n",
        "# Predict the labels by selecting the class with the highest probability\n",
        "y_pred = outputs.predictions.argmax(1)\n",
        "\n",
        "# Define a function to plot a confusion matrix\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n",
        "    \"\"\"\n",
        "    This function plots a confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "        cm (array-like): Confusion matrix as returned by sklearn.metrics.confusion_matrix.\n",
        "        classes (list): List of class names, e.g., ['Class 0', 'Class 1'].\n",
        "        title (str): Title for the plot.\n",
        "        cmap (matplotlib colormap): Colormap for the plot.\n",
        "    \"\"\"\n",
        "    # Create a figure with a specified size\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Display the confusion matrix as an image with a colormap\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Define tick marks and labels for the classes on the axes\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.0f'\n",
        "    # Add text annotations to the plot indicating the values in the cells\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Label the axes\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    # Ensure the plot layout is tight\n",
        "    plt.tight_layout()\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "# Calculate accuracy and F1 score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Display accuracy and F1 score\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Get the confusion matrix if there are a small number of labels\n",
        "if len(labels_list) <= 150:\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot the confusion matrix using the defined function\n",
        "    plot_confusion_matrix(cm, labels_list, figsize=(8, 6))\n",
        "\n",
        "# Finally, display classification report\n",
        "print()\n",
        "print(\"Classification report:\")\n",
        "print()\n",
        "print(classification_report(y_true, y_pred, target_names=labels_list, digits=4))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:19:36.011152Z",
          "iopub.execute_input": "2024-04-06T10:19:36.011428Z",
          "iopub.status.idle": "2024-04-06T10:19:36.447314Z",
          "shell.execute_reply.started": "2024-04-06T10:19:36.011403Z",
          "shell.execute_reply": "2024-04-06T10:19:36.446344Z"
        },
        "trusted": true,
        "id": "9SKYyqu6v-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model: This line of code is responsible for saving the model\n",
        "# that has been trained using the trainer object. It will serialize the model\n",
        "# and its associated weights, making it possible to reload and use the model\n",
        "# in the future without the need to retrain it.\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:19:36.448731Z",
          "iopub.execute_input": "2024-04-06T10:19:36.449439Z",
          "iopub.status.idle": "2024-04-06T10:19:37.113931Z",
          "shell.execute_reply.started": "2024-04-06T10:19:36.449398Z",
          "shell.execute_reply": "2024-04-06T10:19:37.113167Z"
        },
        "trusted": true,
        "id": "h896yVsMv-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'pipeline' function from the 'transformers' library.\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline for image classification tasks.\n",
        "# You need to specify the 'model_name' and the 'device' to use for inference.\n",
        "# - 'model_name': The name of the pre-trained model to be used for image classification.\n",
        "# - 'device': Specifies the device to use for running the model (0 for GPU, -1 for CPU).\n",
        "pipe = pipeline('image-classification', model=model_name, device=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:19:37.115063Z",
          "iopub.execute_input": "2024-04-06T10:19:37.115401Z",
          "iopub.status.idle": "2024-04-06T10:19:37.414244Z",
          "shell.execute_reply.started": "2024-04-06T10:19:37.11537Z",
          "shell.execute_reply": "2024-04-06T10:19:37.413468Z"
        },
        "trusted": true,
        "id": "Rau6jZ9sv-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing an image from the 'test_data' dataset using index 1.\n",
        "image = test_data[1][\"image\"]\n",
        "\n",
        "# Displaying the 'image' variable.\n",
        "image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:19:37.415226Z",
          "iopub.execute_input": "2024-04-06T10:19:37.415489Z",
          "iopub.status.idle": "2024-04-06T10:19:37.425097Z",
          "shell.execute_reply.started": "2024-04-06T10:19:37.415466Z",
          "shell.execute_reply": "2024-04-06T10:19:37.424281Z"
        },
        "trusted": true,
        "id": "3HlMOvLIv-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the 'pipe' function to process the 'image' variable.\n",
        "pipe(image)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:19:37.429413Z",
          "iopub.execute_input": "2024-04-06T10:19:37.429785Z",
          "iopub.status.idle": "2024-04-06T10:19:37.46406Z",
          "shell.execute_reply.started": "2024-04-06T10:19:37.429759Z",
          "shell.execute_reply": "2024-04-06T10:19:37.46319Z"
        },
        "trusted": true,
        "id": "kGmcKBVav-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line of code accesses the \"label\" attribute of a specific element in the test_data list.\n",
        "# It's used to retrieve the actual label associated with a test data point.\n",
        "id2label[test_data[1][\"label\"]]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:19:37.465188Z",
          "iopub.execute_input": "2024-04-06T10:19:37.46552Z",
          "iopub.status.idle": "2024-04-06T10:19:37.473683Z",
          "shell.execute_reply.started": "2024-04-06T10:19:37.465485Z",
          "shell.execute_reply": "2024-04-06T10:19:37.472928Z"
        },
        "trusted": true,
        "id": "feduI4EJv-8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send model to Huggingface"
      ],
      "metadata": {
        "id": "frzPc3OWv-8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary module to interact with the Hugging Face Hub.\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Perform a login to the Hugging Face Hub.\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:21:31.594499Z",
          "iopub.execute_input": "2024-04-06T10:21:31.594979Z",
          "iopub.status.idle": "2024-04-06T10:21:31.620896Z",
          "shell.execute_reply.started": "2024-04-06T10:21:31.594942Z",
          "shell.execute_reply": "2024-04-06T10:21:31.62005Z"
        },
        "trusted": true,
        "id": "Ql3a977Iv-8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the HfApi class from the huggingface_hub library.\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Create an instance of the HfApi class.\n",
        "api = HfApi()\n",
        "\n",
        "# Define the repository ID by combining the username \"dima806\" with the model name.\n",
        "repo_id = f\"dima806/{model_name}\"\n",
        "\n",
        "try:\n",
        "    # Attempt to create a new repository on the Hugging Face Model Hub using the specified repo_id.\n",
        "    api.create_repo(repo_id)\n",
        "\n",
        "    # If the repository creation is successful, print a message indicating that the repository was created.\n",
        "    print(f\"Repo {repo_id} created\")\n",
        "except:\n",
        "    # If an exception is raised, print a message indicating that the repository already exists.\n",
        "    print(f\"Repo {repo_id} already exists\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:21:50.066521Z",
          "iopub.execute_input": "2024-04-06T10:21:50.067246Z",
          "iopub.status.idle": "2024-04-06T10:21:50.165731Z",
          "shell.execute_reply.started": "2024-04-06T10:21:50.067209Z",
          "shell.execute_reply": "2024-04-06T10:21:50.164876Z"
        },
        "trusted": true,
        "id": "Nk_sG7j-v-8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading a folder to the Hugging Face Model Hub\n",
        "api.upload_folder(\n",
        "    folder_path=model_name,  # The path to the folder to be uploaded\n",
        "    path_in_repo=\".\",  # The path where the folder will be stored in the repository\n",
        "    repo_id=repo_id,  # The ID of the repository where the folder will be uploaded\n",
        "    repo_type=\"model\",  # The type of the repository (in this case, a model repository)\n",
        "    revision=\"main\" # Revision name\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T10:21:50.510138Z",
          "iopub.execute_input": "2024-04-06T10:21:50.510492Z",
          "iopub.status.idle": "2024-04-06T10:22:21.116607Z",
          "shell.execute_reply.started": "2024-04-06T10:21:50.510461Z",
          "shell.execute_reply": "2024-04-06T10:22:21.115686Z"
        },
        "trusted": true,
        "id": "so2A6Nqyv-8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRou2i_Xv-8B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}